{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3b943bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import autograd.numpy as np\n",
    "from autograd import grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f865f295",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Neural network definition: for classification problems as of now\n",
    "\n",
    "class Neural_Net:\n",
    "    def __init__(self,layers,loss_func):\n",
    "        self.layers = layers\n",
    "        self.num_layers = len(layers)\n",
    "        self.sizes = [x.size for x in self.layers]\n",
    "        self.init_weights()\n",
    "        self.loss_func = loss_func\n",
    "    \n",
    "    #initializes weights\n",
    "    def init_weights(self):\n",
    "        for i in range(self.num_layers-1):\n",
    "            self.layers[i].weights = np.random.rand(self.layers[i].size,\n",
    "                                                   self.layers[i+1].size)\n",
    "            self.layers[i].bias = np.random.rand(self.layers[i+1].size)\n",
    "    \n",
    "    #computes forward pass\n",
    "    def forward_pass(self):\n",
    "        for i in range(self.num_layers):\n",
    "            #if not last layer:\n",
    "            if i != self.num_layers - 1:\n",
    "                #compute new neurons\n",
    "                self.layers[i+1].neurons = (np.dot(self.layers[i].neurons,\n",
    "                                                 self.layers[i].weights) \n",
    "                                            - self.layers[i].bias)\n",
    "                #apply activation to neurons\n",
    "                self.layers[i+1].neurons = self.layers[i].act(self.layers[i+1].neurons)\n",
    "            else:\n",
    "                #just apply activation if last layer\n",
    "                self.layers[i].neurons = self.layers[i].act(self.layers[i].neurons)\n",
    "                \n",
    "    #computes backward pass\n",
    "    def backward_pass(self):\n",
    "        self.forward_pass()\n",
    "        \n",
    "        y_true = np.array([1.0,0,0])\n",
    "        \n",
    "        loss = self.loss_func(self.layers[-1].neurons,y_true)\n",
    "\n",
    "        loss_grad = grad(self.loss_func)\n",
    "        grad_eval = loss_grad(self.layers[-1].neurons,y_true)\n",
    "        \n",
    "        layer_grads = [None] * self.num_layers\n",
    "        layer_grads[-1] = grad_eval * grad(self.layers[-1].act)(self.layers[-1].neurons)\n",
    "        \n",
    "                \n",
    "\n",
    "class FeatureLayer:\n",
    "    def __init__(self, size):\n",
    "        self.size = size\n",
    "        self.act = np.vectorize(lambda x: max(0,x)) #ReLu\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "        self.neurons = np.zeros(size) #must be equal in length to size.\n",
    "        \n",
    "class DenseLayer:\n",
    "    def __init__(self, size):\n",
    "        self.size = size\n",
    "        self.act = np.vectorize(lambda x: max(0,x)) #ReLu\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "        self.neurons = np.zeros(size)\n",
    "\n",
    "class OutputLayer:\n",
    "    def __init__(self, size):\n",
    "        self.size = size\n",
    "        self.neurons = np.zeros(size)\n",
    "        self.act = (lambda x: np.exp(x)/np.exp(x).sum()) #softmax\n",
    "\n",
    "#categorical crossentropy assuming that softmax has already been applied\n",
    "def categorical_crossentropy(y_predicted,y):\n",
    "    return -np.sum(y*np.log(y_predicted))/y.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "68552383",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Grad only applies to real scalar-output functions. Try jacobian, elementwise_grad or holomorphic_grad.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 10\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m#input into network\u001b[39;00m\n\u001b[1;32m      7\u001b[0m nn\u001b[38;5;241m.\u001b[39mlayers[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mneurons \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m1.0\u001b[39m,\u001b[38;5;241m2.0\u001b[39m])\n\u001b[0;32m---> 10\u001b[0m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward_pass\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(nn\u001b[38;5;241m.\u001b[39mlayers[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mneurons)\n",
      "Cell \u001b[0;32mIn[18], line 45\u001b[0m, in \u001b[0;36mNeural_Net.backward_pass\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     42\u001b[0m grad_eval \u001b[38;5;241m=\u001b[39m loss_grad(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mneurons,y_true)\n\u001b[1;32m     44\u001b[0m layer_grads \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers\n\u001b[0;32m---> 45\u001b[0m layer_grads[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m grad_eval \u001b[38;5;241m*\u001b[39m \u001b[43mgrad\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mneurons\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/autograd/wrap_util.py:20\u001b[0m, in \u001b[0;36munary_to_nary.<locals>.nary_operator.<locals>.nary_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     19\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(args[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m argnum)\n\u001b[0;32m---> 20\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43munary_operator\u001b[49m\u001b[43m(\u001b[49m\u001b[43munary_f\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mnary_op_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mnary_op_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/autograd/differential_operators.py:30\u001b[0m, in \u001b[0;36mgrad\u001b[0;34m(fun, x)\u001b[0m\n\u001b[1;32m     28\u001b[0m vjp, ans \u001b[38;5;241m=\u001b[39m _make_vjp(fun, x)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m vspace(ans)\u001b[38;5;241m.\u001b[39msize \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 30\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGrad only applies to real scalar-output functions. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     31\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTry jacobian, elementwise_grad or holomorphic_grad.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m vjp(vspace(ans)\u001b[38;5;241m.\u001b[39mones())\n",
      "\u001b[0;31mTypeError\u001b[0m: Grad only applies to real scalar-output functions. Try jacobian, elementwise_grad or holomorphic_grad."
     ]
    }
   ],
   "source": [
    "nn = Neural_Net([FeatureLayer(2),\n",
    "                 DenseLayer(4),\n",
    "                 OutputLayer(3)],\n",
    "               categorical_crossentropy)\n",
    "\n",
    "#input into network\n",
    "nn.layers[0].neurons = np.array([1.0,2.0])\n",
    "\n",
    "\n",
    "nn.backward_pass()\n",
    "\n",
    "print(nn.layers[-1].neurons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af59ddb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = Neural_Net([FeatureLayer(2),\n",
    "                 DenseLayer(4),\n",
    "                 OutputLayer(3)])\n",
    "\n",
    "nn.init_weights()\n",
    "\n",
    "print(nn.layers[0].weights)\n",
    "print(nn.layers[1].weights)\n",
    "\n",
    "print(nn.forward_pass())\n",
    "#what is a NN? -> weights, layers, bias, output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d92a927",
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.shape -> (rows,columns)\n",
    "\n",
    "#basic -> input shape = (2,)\n",
    "\n",
    "feature = np.random.rand(2)\n",
    "weights = np.random.rand(2,4)\n",
    "bias = np.random.rand(4)\n",
    "\n",
    "output = np.dot(feature,weights) - bias\n",
    "\n",
    "relu = np.vectorize(lambda x: max(0,x))\n",
    "\n",
    "output = relu(output)\n",
    "output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
